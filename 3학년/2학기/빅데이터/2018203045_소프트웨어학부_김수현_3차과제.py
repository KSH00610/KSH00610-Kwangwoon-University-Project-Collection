# -*- coding: utf-8 -*-
"""KW_MMDS - Colab 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JkhtBXSOK-JgNasrWW3asFtyUqjQcdad

# CS246 - Colab 3
## K-Means & PCA

### Setup

Let's set up Spark on your Colab environment.  Run the cell below!
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""Now we import some of the libraries usually needed by our workload.




"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import pyspark
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf

"""Let's initialize the Spark context."""

# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

"""### Data Preprocessing

In this Colab, rather than downloading a file from Google Drive, we will load a famous machine learning dataset, the [Breast Cancer Wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), using the ```scikit-learn``` datasets loader.
"""

from sklearn.datasets import load_breast_cancer
breast_cancer = load_breast_cancer()

"""For convenience, given that the dataset is small, we first 

*   construct a Pandas dataframe
*   tune the schema
*   and convert it into a Spark dataframe.
"""

pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)
df = spark.createDataFrame(pd_df)

def set_df_columns_nullable(spark, df, column_list, nullable=False):
    for struct_field in df.schema:
        if struct_field.name in column_list:
            struct_field.nullable = nullable
    df_mod = spark.createDataFrame(df.rdd, df.schema)
    return df_mod

df = set_df_columns_nullable(spark, df, df.columns)
df = df.withColumn('features', array(df.columns))
vectors = df.rdd.map(lambda row: Vectors.dense(row.features))

df.printSchema()

"""With the next cell, we build the two data structures that we will be using throughout this Colab:


*   ```features```, a dataframe of Dense vectors, containing all the original features in the dataset;
*   ```labels```, a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not.


"""

from pyspark.ml.linalg import Vectors
features = spark.createDataFrame(vectors.map(Row), ["features"])
labels = pd.Series(breast_cancer.target)

"""### Your task

1. [K-means](https://spark.apache.org/docs/latest/ml-clustering.html) ($k$=2, seed=1) 알고리즘을 사용하여 데이터를 군집화하고, [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering))를 출력하세요.

If you run successfully the Setup and Data Preprocessing stages, you are now ready to cluster the data with the [K-means](https://spark.apache.org/docs/latest/ml-clustering.html) algorithm included in MLlib (Spark's Machine Learning library).
Set the ```k``` parameter to **2**, fit the model, and the compute the [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) (i.e., a measure of quality of the obtained clustering).  

**IMPORTANT:** use the MLlib implementation of the Silhouette score (via ```ClusteringEvaluator```).
"""

# YOUR CODE HERE
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator


# Trains a k-means model.
kmeans = KMeans().setK(2).setSeed(1)
model = kmeans.fit(features)

# Make predictions
predictions = model.transform(features)

# Evaluate clustering by computing Silhouette score
evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))

"""2. 군집화 결과와 ```labels```를 비교하여 얼마나 많은 data points가 올바르게 군집화 됐는지 출력하세요.

Take the predictions produced by K-means, and compare them with the ```labels``` variable (i.e., the ground truth from our dataset).  

Compute how many data points in the dataset have been clustered correctly (i.e., positive cases in one cluster, negative cases in the other).

*HINT*: you can use ```np.count_nonzero(series_a == series_b)``` to quickly compute the element-wise comparison of two series.

**IMPORTANT**: K-means is a clustering algorithm, so it will not output a label for each data point, but just a cluster identifier!  As such, label ```0``` does not necessarily match the cluster identifier ```0```.

"""

# YOUR CODE HERE
pd_pred = predictions.toPandas()

cnt = 0
for i in range(0, 569):
  if pd_pred.prediction[i] != labels[i]:
    cnt+=1

print(cnt)
#np.count_nonzero(정답과 pd_pred["prediction"] 비교)

"""3. PCA로 차원을 축소하여 features를 2차원 데이터로 변환한 후 각 data points를 scatter plot으로 그려보세요.

Now perform dimensionality reduction on the ```features``` using the [PCA](https://spark.apache.org/docs/latest/ml-features.html#pca) statistical procedure, available as well in MLlib.

Set the ```k``` parameter to **2**, effectively reducing the dataset size of a **15X** factor.
"""

# YOUR CODE HERE
from pyspark.ml.feature import PCA
from pyspark.ml.linalg import Vectors

pca = PCA(k=2, inputCol="features", outputCol="pcaFeatures")
model = pca.fit(features)

result = model.transform(features).select("pcaFeatures")
result.show(truncate=False)

# pandas DataFrame으로 변환
pd_res = result.toPandas()

# Vector로 묶여 있는 두 개의 점을 'x'와 'y'로 분리하여 기존의 DataFrame에 추가
pd_res[['x', 'y']] = pd.DataFrame(pd_res.pcaFeatures.tolist(), index=pd_res.index)

"""4. 차원축소된 데이터셋으로 [K-means](https://spark.apache.org/docs/latest/ml-clustering.html) ($k$=2, seed=1)를 수행하고, [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering))를 출력하세요.

Now run K-means with the same parameters as above, but on the ```pcaFeatures``` produced by the PCA reduction you just executed.

Compute the Silhouette score, as well as the number of data points that have been clustered correctly.
"""

# K-means는 'features' 이름을 지닌 열에 대해 동작하므로 열의 이름을 수정
features_2d = result.withColumnRenamed('pcaFeatures', 'features') # 필요한 경우 result 변수를 다른 것으로 수정

# YOUR CODE HERE
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator


# Trains a k-means model.
kmeans = KMeans().setK(2).setSeed(1)
model = kmeans.fit(features_2d)

# Make predictions
predictions = model.transform(features_2d)

# Evaluate clustering by computing Silhouette score
evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))

"""5. 차원이 축소된 데이터셋의 군집화 결과를 scatter plot으로 그려보세요."""

# YOUR CODE HERE
import numpy as np
import matplotlib.pyplot as plt

plt.scatter(pd_res.x, pd_res.y, c = pd_pred.prediction, alpha=0.25) # x 좌표, y 좌표, 컬러 (0 또는 1이 기재된 배열), 투명도
plt.show()

"""6. 차원이 축소된 상태의 데이터셋을 실제 labels를 이용하여 scatter plot으로 그려보세요."""

# YOUR CODE HERE
plt.scatter(pd_res.x, pd_res.y, c=labels, alpha=0.25) # x 좌표, y 좌표, 컬러 (0 또는 1이 기재된 배열열), 투명도
plt.show()

"""수고하셨습니다!"""